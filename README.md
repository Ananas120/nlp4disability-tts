# Practical Study of Deep Learning Models for Speech Synthesis - NLP4Disability workshop

## Introduction

This repository contains the evaluation scripts given to the rates to evaluate the MOS, BMOS and LMOS metrics. The code of the models used as well as the training scripts\* is available [on this repo](https://github.com/yui-mhcp/text_to_speech) [1].

- **Author**    : Langlois Quentin, INGI, ICTEAM, UCLouvain, Belgium
- **Promotor**  : Jodogne Sébastien, INGI, ICTEAM, UCLouvain, Belgium

**Abstract :** Speech synthesis systems, also known as Text-To-Speech (TTS) systems, are increasingly frequent nowadays with multiple applications, such as voice assistants for visually impaired or blind people. These applications require strong real-time capabilities to be usable in practice, which can be at the cost of a reduced quality in the synthesized voices. Furthermore, Deep Learning models, which have shown impressive results in the task of audio generation, are hardly ever used for everyday TTS because of their high demand in computational resources. Training such models also require a large amount of good quality data, which is not available for most languages. This paper explores the benefits of cross-lingual transfer learning, both in terms of training time and of amount of data that is required to obtain good quality models. Our contributions are evaluated with respect to other TTS systems available for the French language.


\* I have simply adapted the available training script by changing the dataset and the train_size / processing parameters according to the model under study.

## Project structure

```bash
├── evaluation_scripts          : the directory sent to the raters
│   ├── audios                      : the directory containing the audios to evaluate
│   │   ├── bmos
│   │   │   ├── audio_0.mp3             : human speaker audio
│   │   │   ├── audio_1000.mp3          : audio from Male-1000 model
│   │   │   ├── audio_100.mp3           : audio from Male-1000 model
│   │   │   ├── audio_24.mp3            : audio from Female-24 model
│   │   │   ├── audio_250.mp3           : audio from Female-250 model
│   │   │   ├── audio_9000.mp3          : audio from Female-3000 model
│   │   │   ├── audio_ttsfree.mp3       : audio from TTS Free [3]
│   │   │   └── audio_ttsmp3.mp3        : audio from TTS MP3 [2]
│   │   ├── fin.mp3                 : audio displayed at the end of the evaluation to thank the rater ;)
│   │   └── mos
│   │       ├── kaggle
│   │           ├── audio_*_0.mp3   : original audio (from the dataset)
│   │           ├── audio_*_1.mp3   : inverted ground truth spectrogram
│   │           ├── audio_*_2.mp3   : audio from model 1
│   │           └── audio_*_3.mp3   : audio from model 2
│   │       └── siwis
│   │           ├── audio_*_0.mp3   : original audio (from the dataset)
│   │           ├── audio_*_1.mp3   : inverted ground truth spectrogram
│   │           ├── audio_*_2.mp3   : audio from model 1
│   │           └── audio_*_3.mp3   : audio from model 2
│   └── evaluation.ipynb            : the evaluation script
├── LICENSE                     : LICENCE of this repository
├── results
│   └── result_*.json           : the result file generated by the evaluation script
├── result_visualization.ipynb
└── utils                       : plot utilities from [1]
```

## References and citation

No citation for this paper yet

- [1] The utilities to plot + the code for the models training : [repo](https://github.com/yui-mhcp/text_to_speech)
- [2] TTS MP3 synthesis : [link](https://ttsmp3.com/text-to-speech/French/)
- [3] TTS Free synthesis : [link](https://ttsfree.com/text-to-speech/french)
